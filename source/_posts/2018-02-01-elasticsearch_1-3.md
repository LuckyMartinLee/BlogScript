---
title: Elasticsearch从入门到放弃(三) -- 疑难配置详解
date: 2018-02-01 15:41:28
tags:
- Elasticsearch
---

## elasticsearch.yml

node.master: true
指定该节点是否有资格被选举成为node，默认是true，es是默认集群中的第一台机器为master，如果这台机挂了就会重新选举master

node.data: true
指定该节点是否存储索引数据，默认为true。

path.data: /path/to/data
设置索引数据的存储路径，默认是es根目录下的data文件夹，可以设置多个存储路径，用逗号隔开，例：
path.data: /path/to/data1,/path/to/data2
但是我们同一个分片的数据会放在同一个路径

index.number_of_shards: 5
设置默认索引分片个数，默认为5片。分片个数是索引创建后一次生成的,后续不可更改设置

index.number_of_replicas: 1
设置默认索引每个分片副本个数，默认每个分片1个副本，如分片数为5，那么副本分片也为5个，总共10个分片。可以通过API去实时修改设置的。（集群健康度可用 curl 'localhost:9200/_cat/health?v' 查看， 分为绿色、黄色或红色。绿色代表一切正常，集群功能齐全，黄色意味着所有的数据都是可用的，但是某些副本没有被分配，红色则代表因为某些原因，某些数据不可用)

bootstrap.mlockall: true
设置为true来锁住内存。因为当jvm开始swapping时es的效率 会降低，所以要保证它不swap，可以把ES_MIN_MEM和ES_MAX_MEM两个环境变量设置成同一个值，并且保证机器有足够的内存分配给es。 同时也要允许elasticsearch的进程可以锁住内存，linux下可以通过ulimit -l unlimited命令。

network.bind_host: 192.168.0.1
设置该节点绑定的ip地址，可以绑定多个ip地址，允许哪些ip可以访问这个这节，包括外部访问和es集群内节点互相访问，可以是ipv4或ipv6的，默认为0.0.0.0。
network.bind_host: ["192.168.0.1","10.210.32.xx"]
代表集群节点之间 使用 192.168.0.1 ip交换数据，同时 允许所有10.210.32.xx IP段 访问集群

network.publish_host: 192.168.0.1
设置es节点之间交互的ip地址，如果不设置它会自动判断，值必须是个真实的ip地址。

network.host: 192.168.0.1
这个参数是用来同时设置bind_host和publish_host上面两个参数，项目局域网内可以使用者一个参数代替上面两个参数配置，但如涉及到内网，外网都要访问es集群，就需要单独设置上面两个参数

transport.tcp.port: 9300
设置节点间交互的tcp端口，默认是9300。

transport.tcp.compress: true
设置在节点间传输数据时是否压缩，默认为false，不压缩

http.max_content_length:100mb
设置请求返回内容的最大容量,默认100mb。

discovery.zen.minimum_master_nodes: 1
设置在选举Master节点时需要参与的最少的候选主节点数，默认为1. 
这个配置就是告诉 Elasticsearch 当没有足够 master 候选节点的时候，就不要进行 master 节点选举，等 master 候选节点足够了才进行选举。
如果使用默认值，则当网络不稳定时有可能会出现脑裂(一种两个主节点同时存在于一个集群的现象)。合理的数值为(master_eligible_nodes/2)+1，其中master_eligible_nodes表示集群中的候选主节点数。

discovery.zen.ping.timeout: 3s
设置在集群中自动发现其他节点时ping连接的超时时间，默认为3秒。在较差的网络环境下需要设置得大一点，防止因误判该节点的存活状态而导致分片的转移。

gateway.recover_after_nodes: 8
设置整个集群提供服务之前你希望有多少个节点在线。看下面的例子：

想象一下假设你有 10 个节点，每个节点只保存一个分片，这些分片有 5 个主分片和5 个副本分片的索引。有时你需要为整个集群做离线维护（比如，为了安装一个新的驱动程序）， 当你重启你的集群，恰巧出现了 5 个节点已经启动，还有 5 个还没启动的场景。
假设其它 5 个节点出问题，或者他们根本没有收到立即重启的命令。不管什么原因，你有 5 个节点在线上，这五个节点会相互通信，选出一个 master，从而形成一个集群。 他们注意到数据不再均匀分布，因为有 5 个节点在集群中丢失了，所以他们之间会立即启动分片复制。
最后，你的其它 5 个节点打开加入了集群。这些节点会发现 它们 的数据正在被复制到其他节点，所以他们删除本地数据（因为这份数据要么是多余的，要么是过时的）。 然后整个集群重新进行平衡，因为集群的大小已经从 5 变成了 10。
在整个过程中，你的节点会消耗磁盘和网络带宽，来回移动数据，因为没有更好的办法。对于有 TB 数据的大集群, 这种无用的数据传输需要 很长时间 。如果等待所有的节点重启好了，整个集群再上线，所有的本地的数据都不需要移动。

这种情况下，我们设置为 8，这意味着至少要有 8 个节点，该集群才可用。

gateway.expected_nodes: 10
设置这个集群中节点的数量，默认为2

gateway.recover_after_time: 5m
设置初始化数据恢复进程的超时时间，默认是5分钟

以上三个配置参数，告诉ES 集群：
等待集群至少存在 8 个节点
等待 5 分钟，或者10 个节点上线后，才进行数据恢复，这取决于哪个条件先达到
这样集群重启的时候避免过多的分片交换。这可能会让数据恢复从数个小时缩短为几秒钟。

discovery.zen.ping.unicast.hosts: ["host1", "host2:port", "host3[portX-portY]"]
设置集群中master节点的初始列表，可以通过这些节点来自动发现新加入集群的节点。
没有任何网络配置，Elasticsearch将绑定到可用的回环地址，并将扫描端口9300到9305以尝试连接到在同一服务器上运行的其他节点。 这提供了自动群集体验，无需进行任何配置。

## jvm heap size 配置

一般在运行elasticsearch 的时候最小需要是内存是1G，少于1G我们会经常启动不了。
-Xms1g  # 最小值为1G
-Xmx1g  # 最大值为1G
对于这个值的设置，官方为了适应不同的java版本,特做了一些适应配置
-Xms1g   不受版本影响，默认
8:-Xmx2g   只适应java8版本
8-:-Xmx2g  适应java8及以上版本
8-9:-Xmx2g  适应java8-java9版本

官方文档写到：以往经验得出，
1. 最大值和最小值设置为一样的值，否则在系统使用的时候会因jvm值变化而导致服务暂停
2. 过多的内存，会导致用于缓存的内存越多，最终导致回收内存的时间也加长
3. 设置的内存不要超过物理内存的50%，以保证有足够的内存留给操作系统
4. 不要将内存设置超过32GB